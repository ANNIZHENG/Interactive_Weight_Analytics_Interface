{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2180a0d9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbe2cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b480498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve所有video的名字\n",
    "\n",
    "frames_path = 'data/frames/'\n",
    "frames_path_normal = 'data/frames/Normal/'\n",
    "frames_path_crash = 'data/frames/Crash/'\n",
    "\n",
    "frames_name_normal = sorted([f for f in os.listdir(frames_path_normal)])\n",
    "frames_name_crash = sorted([f for f in os.listdir(frames_path_crash)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa0b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我只要50个video做train test split\n",
    "\n",
    "frames_name_normal = frames_name_normal[1:26]\n",
    "frames_name_crash = frames_name_crash[1:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009c6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三七分 train test split\n",
    "\n",
    "train_normal, test_normal = train_test_split(frames_name_normal, \\\n",
    "                                     test_size=0.3, \\\n",
    "                                     random_state=42)\n",
    "\n",
    "train_crash, test_crash = train_test_split(frames_name_crash, \\\n",
    "                                     test_size=0.3, \\\n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48bef568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `##` --> Adjustable\n",
    "\n",
    "# DATA\n",
    "IMG_SIZE = 128  ## Image size (128, 128) in this case\n",
    "CHAN_SIZE = 1   ## 1 - GrayScale; 3 - RGB\n",
    "BATCH_SIZE = 8  ## 教程给的是 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2  ## Crash vs. Normal\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 10  ## 教程给的好像是 100\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 64  ## size of the feature vectors transformed from the input\n",
    "NUM_HEADS = 4        ## 教程给的是 8\n",
    "NUM_LAYERS = 6       ## 教程给的是 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHAN_SIZE)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    # image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "train_videos = []\n",
    "test_videos = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "for t in train_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for t in test_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(0)\n",
    "\n",
    "for t in train_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for t in test_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17a65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.asarray(train_videos)\n",
    "test_videos = np.asarray(test_videos)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "# print(len(train_videos), len(train_labels)) # 34, 34\n",
    "# print(len(test_videos), len(test_labels)) # 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74b6958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20728d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = ops.arange(0, num_tokens, 1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793a9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=ops.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=ops.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9697b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.8624 - loss: 0.5217 - top-5-accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6093 - loss: 0.7132 - top-5-accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.8138 - loss: 0.4698 - top-5-accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7925 - loss: 0.5186 - top-5-accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7324 - loss: 0.5481 - top-5-accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.7631 - loss: 0.3793 - top-5-accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.7930 - loss: 0.4525 - top-5-accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7526 - loss: 0.5642 - top-5-accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8676 - loss: 0.3875 - top-5-accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7983 - loss: 0.3744 - top-5-accuracy: 1.0000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 900ms/step - accuracy: 0.6250 - loss: 0.3855 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 68.75%\n",
      "Test top accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "                                                    # 先不要validloader啦\n",
    "    history = model.fit(trainloader, epochs=EPOCHS) # validation_data=validloader\n",
    "\n",
    "    loss, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {accuracy * 100}%\")\n",
    "    print(f\"Test top accuracy: {top_5_accuracy * 100}%\")\n",
    "\n",
    "    return model,history\n",
    "\n",
    "\n",
    "model,history = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96dc69",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05c39734",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Accuracy and Loss\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3acee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Feature Heatmap using attention weights\n",
    "\n",
    "weights_per_layer = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights_per_layer.append(layer.get_weights())\n",
    "    \n",
    "    # Attention Weights\n",
    "    \n",
    "    # These scores represent how much each part of the input image (divided into patches) \n",
    "    # attends to every other part (relations between layers)\n",
    "    \n",
    "    # Transformers typically use multi-headed attention mechanisms where each head can potentially \n",
    "    # focus on different features or parts of the image.\n",
    "    \n",
    "    # To visualize these as heatmaps over the original image, \n",
    "    #you will need to reshape these matrices back to the spatial dimensions of the image \n",
    "    # and use interpolation techniques to upscale the coarse heatmap to the full resolution of the image.\n",
    "    \n",
    "    # Visualize the Matrices: Use heatmaps to visualize each matrix. Areas of higher values (e.g., closer to 1)\n",
    "    # indicate stronger attention and can be represented with warmer colors.\n",
    "    \n",
    "    # Calculate the average attention that each frame gives to every other frame. \n",
    "    # This can help identify if certain frames are particularly influential or are being ignored.\n",
    "    \n",
    "    # Look at the rows or columns that correspond to specific patches. \n",
    "    # For instance, if a particular patch in Frame 1 consistently shows high attention values \n",
    "    # across its row when looking at other frames, this indicates that the patch is of high interest \n",
    "    # across the video.\n",
    "    \n",
    "    # Patches are fixed-size, non-overlapping segments of the original image \n",
    "    # (or frame in the case of video), each treated as a distinct element.\n",
    "    \n",
    "weights_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FOR REFERENCE \"\"\"\n",
    "\n",
    "# DATA\n",
    "IMG_SIZE = 128  ## Image size (128, 128) in this case\n",
    "CHAN_SIZE = 1   ## 1 - GrayScale; 3 - RGB\n",
    "BATCH_SIZE = 8  ## 教程给的是 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2  ## Crash vs. Normal\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 10  ## 教程给的好像是 100\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 64  ## size of the feature vectors transformed from the input\n",
    "NUM_HEADS = 4        ## 教程给的是 8\n",
    "NUM_LAYERS = 6       ## 教程给的是 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
