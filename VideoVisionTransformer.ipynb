{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79365ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ad872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve所有video的名字\n",
    "\n",
    "frames_path = 'data/frames/'\n",
    "frames_path_normal = 'data/frames/Normal/'\n",
    "frames_path_crash = 'data/frames/Crash/'\n",
    "\n",
    "frames_name_normal = sorted([f for f in os.listdir(frames_path_normal)])\n",
    "frames_name_crash = sorted([f for f in os.listdir(frames_path_crash)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0434b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我只要50个video做train test split\n",
    "\n",
    "frames_name_normal = frames_name_normal[1:26]\n",
    "frames_name_crash = frames_name_crash[1:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2804dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三七分 train test split\n",
    "\n",
    "train_normal, test_normal = train_test_split(frames_name_normal, \\\n",
    "                                     test_size=0.3, \\\n",
    "                                     random_state=42)\n",
    "\n",
    "train_crash, test_crash = train_test_split(frames_name_crash, \\\n",
    "                                     test_size=0.3, \\\n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790582ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Size\n",
    "\n",
    "img_size = 128\n",
    "chan_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b69d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把每一个图片都变成Tensor形式\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=chan_size)\n",
    "    image = tf.image.resize(image, [img_size, img_size])\n",
    "    # image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "# train_normal_videos = []\n",
    "# test_normal_videos = []\n",
    "# train_crash_videos = []\n",
    "# test_crash_videos = []\n",
    "\n",
    "train_videos = []\n",
    "test_videos = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "for t in train_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for t in test_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(0)\n",
    "\n",
    "for t in train_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for t in test_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t \\\n",
    "                                    + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    # print(video.shape) # (50, 224, 224, 3)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f633d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = np.asarray(train_videos)\n",
    "test_videos = np.asarray(test_videos)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "# print(len(train_videos), len(train_labels)) # 34, 34\n",
    "# print(len(test_videos), len(test_labels)) # 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151ae1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 3 ## 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, img_size, img_size, chan_size)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 10\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 64 ## 128\n",
    "NUM_HEADS = 4 ## 8\n",
    "NUM_LAYERS = 6 ## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8068a53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1340b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a8f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = ops.arange(0, num_tokens, 1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec370c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=ops.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=ops.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b2dbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 618ms/step - accuracy: 0.7749 - loss: 1.4232 - top-5-accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 620ms/step - accuracy: 0.4121 - loss: 2.0088 - top-5-accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 627ms/step - accuracy: 0.6203 - loss: 0.8436 - top-5-accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 681ms/step - accuracy: 0.6827 - loss: 0.4344 - top-5-accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 682ms/step - accuracy: 0.8157 - loss: 0.4425 - top-5-accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 644ms/step - accuracy: 0.7932 - loss: 0.4996 - top-5-accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 663ms/step - accuracy: 0.7748 - loss: 0.4500 - top-5-accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 657ms/step - accuracy: 0.7612 - loss: 0.4905 - top-5-accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 664ms/step - accuracy: 0.6800 - loss: 0.5938 - top-5-accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 666ms/step - accuracy: 0.7633 - loss: 0.5048 - top-5-accuracy: 1.0000\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - accuracy: 0.6603 - loss: 0.6268 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 75.0%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS) # validation_data=validloader\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
