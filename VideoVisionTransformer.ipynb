{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2180a0d9",
   "metadata": {},
   "source": [
    "## Video Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe2cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "import matplotlib.pyplot as plt\n",
    "keras.utils.set_random_seed(42)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True) \n",
    "# os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7dedf",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bef568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `##` --> Adjustable\n",
    "\n",
    "# DATA\n",
    "IMG_SIZE = 128  ## Image size (128, 128) in this case\n",
    "CHAN_SIZE = 1   # 1-GrayScale; 3-RGB\n",
    "BATCH_SIZE = 8  ## 16, 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2  # 1-Crash; 0-Normal\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "# EPOCHS = 10\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8,8,8) ##\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "EMBED_DIM = 64   ## Size of the feature vectors transformed from the input\n",
    "NUM_HEADS =  6   ##\n",
    "NUM_LAYERS = 6   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80fc4",
   "metadata": {},
   "source": [
    "## 111 Retrieve videos for training, validating, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b480498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all video name\n",
    "\n",
    "frames_path = 'data/frames/'\n",
    "frames_path_normal = 'data/frames/Normal/'\n",
    "frames_path_crash = 'data/frames/Crash/'\n",
    "\n",
    "frames_name_normal = sorted([f for f in os.listdir(frames_path_normal)])\n",
    "frames_name_crash = sorted([f for f in os.listdir(frames_path_crash)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab11fbc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_normal \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      3\u001b[0m num_crash \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m frames_name_normal \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(frames_name_normal, num_normal)\n\u001b[1;32m      6\u001b[0m frames_name_crash \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(frames_name_crash, num_crash)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "#how many data needed\n",
    "num_normal = 50\n",
    "num_crash = 50\n",
    "\n",
    "frames_name_normal = random.sample(frames_name_normal, num_normal)\n",
    "frames_name_crash = random.sample(frames_name_crash, num_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6:2:2.5 train test split\n",
    "\n",
    "train_normal, test_normal = train_test_split(frames_name_normal,test_size=0.3, random_state=42)\n",
    "train_crash, test_crash = train_test_split(frames_name_crash, test_size=0.3, random_state=42)\n",
    "\n",
    "temporary_normal, test_normal = train_test_split(frames_name_normal, test_size=0.2, random_state=42)\n",
    "temporary_crash, test_crash = train_test_split(frames_name_crash, test_size=0.2, random_state=42)\n",
    "\n",
    "train_normal, val_normal = train_test_split(temporary_normal, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "train_crash, val_crash = train_test_split(temporary_crash, test_size=0.25, random_state=42)    # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal Training Videos:\", train_normal)\n",
    "print(\"Crash Training Videos:\", train_crash)\n",
    "print(\"Normal Validation Videos:\", val_normal)\n",
    "print(\"Crash Validation Videos:\", val_crash)\n",
    "print(\"Normal Test Videos:\", test_normal)\n",
    "print(\"Crash Test Videos:\", test_crash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f3f44",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform image to matrix format\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHAN_SIZE)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # Normalization\n",
    "    return image\n",
    "\n",
    "train_videos = []\n",
    "test_videos = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for t in train_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for t in test_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(0)\n",
    "\n",
    "for t in train_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for t in test_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(1)\n",
    "\n",
    "    \n",
    "valid_videos = []\n",
    "valid_labels = []\n",
    "    \n",
    "for t in frames_name_crash_valid:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    valid_videos.append(video.numpy())\n",
    "    valid_labels.append(1)\n",
    "\n",
    "for t in frames_name_normal_valid:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    valid_videos.append(video.numpy())\n",
    "    valid_labels.append(0)\n",
    "\n",
    "    \n",
    "train_videos = np.asarray(train_videos)\n",
    "test_videos = np.asarray(test_videos)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "valid_videos = np.asarray(valid_videos)\n",
    "valid_labels = np.asarray(valid_labels)\n",
    "\n",
    "# print(len(train_videos), len(train_labels)) # 34, 34\n",
    "# print(len(test_videos), len(test_labels)) # 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "\n",
    "# Create Embedding Mechanism\n",
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        \n",
    "        # `projected_patches`\n",
    "        # dividing the input into patches (determined by kernel_size and strides) \n",
    "        # and transforming each patch into an 64-dimensional embedding.\n",
    "        \n",
    "        projected_patches = self.projection(videos) \n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n",
    "\n",
    "# Create Positional Mechanism\n",
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = ops.arange(0, num_tokens, 1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n",
    "\n",
    "# class CustomMultiHeadAttention(layers.Layer):\n",
    "#     def __init__(self, num_heads, key_dim, dropout=0, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.attention = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads, key_dim=key_dim, dropout=dropout\n",
    "#         )\n",
    "#     def call(self, query, value):\n",
    "#         attention_output, attention_scores = self.attention(query, value, value)\n",
    "#         return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73230f36",
   "metadata": {},
   "source": [
    "## 111 Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" REFERENCE\n",
    "\n",
    "IMG_SIZE = 128\n",
    "CHAN_SIZE = 1\n",
    "BATCH_SIZE = 8\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "PATCH_SIZE = (8,8,8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS =  6\n",
    "NUM_LAYERS = 6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape) # shape=(50,128,128,1)\n",
    "    # Create patches\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        \n",
    "        # 1. Layer normalization and MultiHeadAttention\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        \n",
    "        # 2. The MultiHeadAttention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # 3. Skip connection - Add output from dense layers to earlier layer normalization\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # 4. Layer Normalization and MultiLayerPerception\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # 5. The two fully-connected layers with GELU activation functions\n",
    "        x4 = layers.Dropout(0.1)(layers.Dense(units=embed_dim, activation='gelu')(x3))\n",
    "        x5 = layers.Dropout(0.1)(layers.Dense(units=embed_dim, activation='gelu')(x4))\n",
    "        \n",
    "        # 6. Skip connection - Add output from dense layers to earlier layer normalization\n",
    "        encoded_patches = layers.Add()([x5, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(callback=None):\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=EMBED_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=EMBED_DIM)\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    # You may definn the epochs here\n",
    "    if (callback != None):\n",
    "        history = model.fit(trainloader, epochs=5, validation_data=validloader)\n",
    "    else:\n",
    "        history = model.fit(trainloader, epochs=5, validation_data=validloader, callbacks=callback)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "model, history = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31652028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96dc69",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0381a",
   "metadata": {},
   "source": [
    "## 1. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c39734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and Loss\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b4518",
   "metadata": {},
   "source": [
    "## 2. Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c71b7",
   "metadata": {},
   "source": [
    "### 2.1 Result Evaluation  - Bias towards Classifying Videos to Normal Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate 16 testing videos\n",
    "\n",
    "model_predict_test = model.predict(testloader)\n",
    "\n",
    "# Inspect Test Results\n",
    "\n",
    "pred = []\n",
    "normal_count = 0\n",
    "misclassified_indicies = []\n",
    "\n",
    "for i in range(16):\n",
    "    crash_conf = model_predict_test[i][0]\n",
    "    normal_conf = model_predict_test[i][1]\n",
    "    if (crash_conf > normal_conf):\n",
    "        print(\"Prediction: Crash\", crash_conf)\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        print(\"Prediction: Normal\", normal_conf)\n",
    "        pred.append(0)\n",
    "        normal_count += 1\n",
    "    if (i < 8):\n",
    "        print(\"Actual:     Crash\", test_normal[i]+\".mp4\")\n",
    "    else:\n",
    "        print(\"Actual:     Normal\", test_crash[abs(i-8)]+\".mp4\")\n",
    "    print()\n",
    "print(str(abs(len(test_labels)-normal_count)) + \" videos out of 16 videos are classified as Crash\")\n",
    "print(str(normal_count) + \" videos out of 16 videos are classified as Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5031b",
   "metadata": {},
   "source": [
    "### 2.2 Weight and Bias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac78276",
   "metadata": {},
   "source": [
    "# weights,biases = model.layers[-1].get_weights()<br>这行代码extract了最后一层的features<br>一共64个feature，两个class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057650fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # If interested in knowing what techniques are used for each layer\n",
    "# for layer in model.layers:\n",
    "#     print(layer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf01a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract Weights and Biases from the Two Dense Layers\n",
    "weights,biases = model.layers[-1].get_weights()\n",
    "# print(np.array(weights.shape)) # [64  2], 64 embedding layers with 2 classes\n",
    "\n",
    "# Biases\n",
    "print(\"Biases\")\n",
    "print(np.array(biases))  # [-0.00016644  0.00016644]\n",
    "\n",
    "# Weights\n",
    "print(\"\\nWeights\")\n",
    "pd.DataFrame(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(pd.DataFrame(weights).iloc[:,0]), min(pd.DataFrame(weights).iloc[:,0]), \\\n",
    "      max(pd.DataFrame(weights).iloc[:,1]), min(pd.DataFrame(weights).iloc[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd2db8",
   "metadata": {},
   "source": [
    "### (i) Overall weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "# Weight Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Weight Distribution of the Last Layer\")\n",
    "plt.hist(weights.flatten(), bins=50)\n",
    "plt.xlabel(\"Weight values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Bias Distribution\n",
    "plt.title(\"Bias Distribution from the Last Layer\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(biases.flatten(), bins=50)\n",
    "plt.xlabel(\"Bias values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73325b",
   "metadata": {},
   "source": [
    "### (ii) For each feature, does it contribute POSITIVELY or NEGATIVELY to each of the classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8b50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "num_features = weights.shape[0]\n",
    "\n",
    "# Prepare boundaries for each graph\n",
    "bound = math.ceil(max(abs(np.max(weights)), abs(np.min(weights)))*10)/10\n",
    "\n",
    "# Plotting \n",
    "\n",
    "fig, axs = plt.subplots(num_features, 1, figsize=(5, 55))  #Adjust size as needed\n",
    "\n",
    "for i in range(num_features):\n",
    "    index = np.arange(2)\n",
    "    bar_height = 0.5\n",
    "    offset = np.array([0, bar_height])\n",
    "    axs[i].barh(index-offset, weights[i, :], bar_height, color=['blue', 'orange'])\n",
    "    axs[i].set_yticks(index)\n",
    "    axs[i].set_yticklabels(['Normal', 'Crash'])\n",
    "    axs[i].set_title('Feature {}'.format(i))\n",
    "\n",
    "    # Setting x-axis \n",
    "    axs[i].set_xlim([-bound, bound])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841b8b1",
   "metadata": {},
   "source": [
    "### What can we tell?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764501e4",
   "metadata": {},
   "source": [
    "### (iii) For each feature, which feature does it tend more to contribute to? [Graphs of Comparison]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe38b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the differences: Crash - Normal\n",
    "differences = weights[:, 1] - weights[:, 0]\n",
    "\n",
    "# Number of features\n",
    "num_features = weights.shape[0]\n",
    "\n",
    "# Set bound\n",
    "bound = math.ceil(np.max(differences)*10)/10\n",
    "\n",
    "# Creating the plot\n",
    "fig, axs = plt.subplots(64, 1, figsize=(5, 50))  # Adjust size as needed\n",
    "\n",
    "for i in range(num_features):\n",
    "    axs[i].barh(0, differences[i], color='blue' if differences[i] >= 0 else 'orange')\n",
    "    axs[i].set_title('Feature {}'.format(i))\n",
    "    axs[i].set_xlim([differences.min() - 1, differences.max() + 1])\n",
    "    axs[i].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdedc26",
   "metadata": {},
   "source": [
    "### What can we tell?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff922b",
   "metadata": {},
   "source": [
    "### (iv) What if we use only a few selected features in a selected layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e8fda",
   "metadata": {},
   "source": [
    "# 我在这里提取出我感兴趣的indicies<br>然后设置了感兴趣的layer_index<br>看看如果重新train model的时候（在那个特定的layer只跑特定的weights），会不会出现什么神奇的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44681197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce583aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate the `differences` into those that contribute more to the normal class or crash class\n",
    "tend_normal = abs(differences[differences<0])\n",
    "tend_crash = abs(differences[differences>0])\n",
    "\n",
    "# Select a number\n",
    "x = 2\n",
    "\n",
    "# Select the top x features' indicies that contribute more to either of the class\n",
    "tend_normal_top_x_indices = np.argsort(tend_normal)[-x:][::-1]\n",
    "tend_crash_top_x_indices = np.argsort(tend_crash)[-x:][::-1]\n",
    "selected_weights_indices = np.concatenate((tend_normal_top_x_indices, tend_crash_top_x_indices))\n",
    "# selected_weights = weights[selected_weights_indices]\n",
    "\n",
    "# Biases\n",
    "# biases\n",
    "\n",
    "# I have the biases and the weight matrix for binary classification. \n",
    "# How should I apply the GELU with those information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1b674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ApplyMaskCallback(Callback):\n",
    "    def __init__(self, mask, layer_index):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.layer_index = layer_index\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        weights = self.model.layers[self.layer_index].get_weights()\n",
    "        weights[0] *= self.mask  # Apply mask to weights, not biases\n",
    "        self.model.layers[self.layer_index].set_weights(weights)\n",
    "\n",
    "layer_index = -1 # last layer\n",
    "mask = np.ones_like(model.layers[layer_index].get_weights()[0])\n",
    "mask = np.zeros((EMBED_DIM, NUM_CLASSES))\n",
    "mask[selected_weights_indices, :] = 1\n",
    "callback = ApplyMaskCallback(mask, layer_index=layer_index)\n",
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e798fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new, history_new = run_experiment(callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc20145",
   "metadata": {},
   "source": [
    "**Why using callback?**\n",
    "\n",
    "Selectively zero out weights in a specific layer (last layer) of a neural network during training\n",
    "\n",
    "Allowing the model to only learn using the unmasked weights during that last layer. \n",
    "\n",
    "This is useful for testing the impact of specific features or weights on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49244d03",
   "metadata": {},
   "source": [
    "### 2.3 Confusion Matrix (TF vs. TN vs. FP vs. FN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "true = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0]\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Labels (0-Normal, 1-Crash)')\n",
    "plt.ylabel('True Labels (0-Normal, 1-Crash)')\n",
    "plt.title('Confusion Matrix of Predicted vs. True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d10de8",
   "metadata": {},
   "source": [
    "### 2.4 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the indicies of the misclassified test videos\n",
    "misclassified = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] != true[i]:\n",
    "        misclassified.append(i)\n",
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e91d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we visualize it?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Attention Scores Calculation: \n",
    "The attention mechanism calculates scores by performing a dot product of the query with all keys for each head.\n",
    "These scores determine how much focus (or attention) should be given to other parts of the input.\n",
    "\n",
    "Aggregated Information: Each element in the output tensor represents an aggregation of information \n",
    "from other parts of the input, weighted by the calculated attention scores.\n",
    "\n",
    "\n",
    "You can aggregate attention weights across different frames to see \n",
    "if there's a temporal pattern in how the model attends to different regions of the video.\n",
    "\n",
    "Generate heatmaps for individual frames \n",
    "or average them across a sequence to visually \n",
    "represent where the model is \"looking.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# attention.shape  # (1, 1536, 64) \n",
    "# (Batch Size (1), Num of Tokens (1526), Embedding Dimension (64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" REFERENCE\n",
    "\n",
    "EMBED_DIM = 64\n",
    "PATCH_SIZE = 8,8,8\n",
    "INPUT_SHAPE = (50, 128, 128, 1)\n",
    "NUM_HEADS = 6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# The below codes are similar to the codes in line 33-62 \n",
    "#              in the `create_vivit_classifier` function\n",
    "\n",
    "def preprocess_frames(frames: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors for prediction.\"\"\"\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[..., tf.newaxis],\n",
    "        tf.float32,\n",
    "    )\n",
    "    return frames\n",
    "\n",
    "# Define the model up to the attention layer\n",
    "# The below codes are similar to the codes in line 33-62 \n",
    "#              in the `create_vivit_classifier` function\n",
    "\n",
    "inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "tubelet_embedder = TubeletEmbedding(embed_dim=EMBED_DIM, patch_size=PATCH_SIZE)\n",
    "patches = tubelet_embedder(inputs)\n",
    "positional_encoder = PositionalEncoder(embed_dim=EMBED_DIM)\n",
    "encoded_patches = positional_encoder(patches)\n",
    "x = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n",
    "attention_output = multi_head_attention(x, x)\n",
    "\n",
    "# Model Building\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=attention_output)\n",
    "\n",
    "# Assuming test_videos[0] is a numpy array and needs to be converted to a TensorFlow tensor\n",
    "\n",
    "video_tensor = tf.convert_to_tensor(test_videos[0], dtype=tf.float32)\n",
    "preprocessed_video = preprocess_frames(video_tensor)\n",
    "preprocessed_video = tf.expand_dims(preprocessed_video, axis=0)\n",
    "attention = model.predict(preprocessed_video)\n",
    "\n",
    "print(attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
