{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2180a0d9",
   "metadata": {},
   "source": [
    "## Video Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe2cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "import matplotlib.pyplot as plt\n",
    "keras.utils.set_random_seed(42)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True) \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.manifold import (TSNE)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7dedf",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bef568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `##` --> Adjustable\n",
    "\n",
    "# DATA\n",
    "IMG_SIZE = 128  ## Image size (128, 128) in this case\n",
    "CHAN_SIZE = 1   # 1-GrayScale; 3-RGB\n",
    "BATCH_SIZE = 8  ## 16, 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2  # 1-Crash; 0-Normal\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "# EPOCHS = 10\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8,8,8) ##\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "EMBED_DIM = 64   ## Size of the feature vectors transformed from the input\n",
    "NUM_HEADS =  6   ##\n",
    "NUM_LAYERS = 6   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80fc4",
   "metadata": {},
   "source": [
    "## Retrieve videos for training, validating, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b480498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all video name\n",
    "\n",
    "# frames_path = 'data/frames/'\n",
    "# frames_path_normal = 'data/frames/Normal/'\n",
    "# frames_path_crash = 'data/frames/Crash/'\n",
    "\n",
    "frames_path = '../data/frames/'\n",
    "frames_path_normal = '../data/frames/Normal/'\n",
    "frames_path_crash = '../data/frames/Crash/'\n",
    "\n",
    "\n",
    "frames_name_normal = sorted([f for f in os.listdir(frames_path_normal)])\n",
    "frames_name_crash = sorted([f for f in os.listdir(frames_path_crash)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab11fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many data needed\n",
    "num_normal = 50\n",
    "num_crash = 50\n",
    "\n",
    "frames_name_normal = random.sample(frames_name_normal, num_normal)\n",
    "frames_name_crash = random.sample(frames_name_crash, num_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49d8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6:2:2.5 train test split\n",
    "\n",
    "train_normal, test_normal = train_test_split(frames_name_normal,test_size=0.3, random_state=42)\n",
    "train_crash, test_crash = train_test_split(frames_name_crash, test_size=0.3, random_state=42)\n",
    "\n",
    "temporary_normal, test_normal = train_test_split(frames_name_normal, test_size=0.2, random_state=42)\n",
    "temporary_crash, test_crash = train_test_split(frames_name_crash, test_size=0.2, random_state=42)\n",
    "\n",
    "train_normal, val_normal = train_test_split(temporary_normal, test_size=0.25, random_state=42)\n",
    "train_crash, val_crash = train_test_split(temporary_crash, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0b8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Training Videos: ['000103', '000384', '002860', '000815', '000109', '002473', '001719', '002299', '001729', '000178', '000572', '000896', '001004', '001379', '002419', '001139', '000357', '000380', '002772', '000420', '000457', '002070', '002234', '001394', '000654', '001840', '001409', '001140', '002933', '000903']\n",
      "\n",
      "Crash Training Videos: ['001099', '000094', '001400', '000779', '000477', '001311', '000334', '000207', '000394', '001141', '001131', '001355', '000776', '001094', '001183', '001248', '000741', '000502', '001288', '000601', '000941', '000593', '001267', '000147', '001438', '000728', '000553', '001373', '000570', '000759']\n",
      "\n",
      "Normal Validation Videos: ['000915', '000397', '002620', '001557', '001127', '002662', '000123', '001732', '000637', '000419']\n",
      "\n",
      "Crash Validation Videos: ['000162', '000947', '001495', '000335', '000256', '000929', '000143', '001328', '001301', '001494']\n",
      "\n",
      "Normal Test Videos: ['000131', '000882', '002414', '001471', '000953', '001084', '002233', '002873', '000027', '002466']\n",
      "\n",
      "Crash Test Videos: ['001443', '000351', '000430', '000778', '000467', '001410', '000748', '001302', '000547', '000164']\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal Training Videos:\", train_normal)\n",
    "print()\n",
    "print(\"Crash Training Videos:\", train_crash)\n",
    "print()\n",
    "print(\"Normal Validation Videos:\", val_normal)\n",
    "print()\n",
    "print(\"Crash Validation Videos:\", val_crash)\n",
    "print()\n",
    "print(\"Normal Test Videos:\", test_normal)\n",
    "print()\n",
    "print(\"Crash Test Videos:\", test_crash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f3f44",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform image to matrix format\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHAN_SIZE)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # Normalization\n",
    "    return image\n",
    "\n",
    "train_videos = []\n",
    "test_videos = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for t in train_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for t in test_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(0)\n",
    "\n",
    "for t in train_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    train_videos.append(video.numpy())\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for t in test_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    test_videos.append(video.numpy())\n",
    "    test_labels.append(1)\n",
    "\n",
    "train_videos = np.asarray(train_videos)\n",
    "test_videos = np.asarray(test_videos)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "valid_videos = []\n",
    "valid_labels = []\n",
    "    \n",
    "for t in val_crash:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_crash + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    valid_videos.append(video.numpy())\n",
    "    valid_labels.append(1)\n",
    "\n",
    "for t in val_normal:\n",
    "    video = []\n",
    "    for i in range(50):\n",
    "        current_frame_index = str(i)\n",
    "        if (i < 10):\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_000\" + str(i) + \".jpg\"))\n",
    "        else:\n",
    "            video.append(load_image(frames_path_normal + t + \"/frame_00\" + str(i) + \".jpg\"))\n",
    "    video = tf.stack(video)\n",
    "    valid_videos.append(video.numpy())\n",
    "    valid_labels.append(0)\n",
    "\n",
    "valid_videos = np.asarray(valid_videos)\n",
    "valid_labels = np.asarray(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "\n",
    "# Create Embedding Mechanism\n",
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        \n",
    "        # `projected_patches`\n",
    "        # dividing the input into patches (determined by kernel_size and strides) \n",
    "        # and transforming each patch into an 64-dimensional embedding.\n",
    "        \n",
    "        projected_patches = self.projection(videos) \n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n",
    "\n",
    "# Create Positional Mechanism\n",
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = ops.arange(0, num_tokens, 1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73230f36",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793a9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" REFERENCE\n",
    "\n",
    "IMG_SIZE = 128\n",
    "CHAN_SIZE = 1\n",
    "BATCH_SIZE = 8\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (50, IMG_SIZE, IMG_SIZE, CHAN_SIZE)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "PATCH_SIZE = (8,8,8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS =  6\n",
    "NUM_LAYERS = 6\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape) # shape=(50,128,128,1)\n",
    "    # Create patches\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        \n",
    "        # 1. Layer normalization and MultiHeadAttention\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        \n",
    "        # 2. The MultiHeadAttention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # 3. Skip connection - Add output from dense layers to earlier layer normalization\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # 4. Layer Normalization and MultiLayerPerception\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # 5. The two fully-connected layers with GELU activation functions\n",
    "        x4 = layers.Dropout(0.1)(layers.Dense(units=embed_dim, activation='gelu')(x3))\n",
    "        x5 = layers.Dropout(0.1)(layers.Dense(units=embed_dim, activation='gelu')(x4))\n",
    "        \n",
    "        # 6. Skip connection - Add output from dense layers to earlier layer normalization\n",
    "        encoded_patches = layers.Add()([x5, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9697b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks being passed to model.fit(): None\n",
      "Epoch 1/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 46s/step - accuracy: 0.7388 - loss: 0.9468 - top-5-accuracy: 1.0000 - val_accuracy: 0.5000 - val_loss: 0.6920 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 61s/step - accuracy: 0.7852 - loss: 0.5949 - top-5-accuracy: 1.0000 - val_accuracy: 0.5500 - val_loss: 0.7346 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 87s/step - accuracy: 0.3423 - loss: 0.8312 - top-5-accuracy: 1.0000 - val_accuracy: 0.6000 - val_loss: 0.6445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 86s/step - accuracy: 0.5775 - loss: 0.6886 - top-5-accuracy: 1.0000 - val_accuracy: 0.6500 - val_loss: 0.6039 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 86s/step - accuracy: 0.5881 - loss: 0.6381 - top-5-accuracy: 1.0000 - val_accuracy: 0.6500 - val_loss: 0.5913 - val_top-5-accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(callbacks=None):\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=EMBED_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=EMBED_DIM)\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    # You may definn the epochs here\n",
    "    print(\"Callbacks being passed to model.fit():\", callbacks)\n",
    "    if (callbacks != None):\n",
    "        history = model.fit(trainloader, epochs=5, validation_data=validloader, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(trainloader, epochs=5, validation_data=validloader)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "model, history = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96dc69",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a2593",
   "metadata": {},
   "source": [
    "Specific on how many feature using, on which layer, we would provide such visualizations.\n",
    "1. Model Evaluation\n",
    "    1. Model accuracy & loss\n",
    "    2. ROC Curves and AUC \n",
    "2. Model Analysis\n",
    "    1. Confusion Matrix\n",
    "    2. Feature analysis\n",
    "        1. Weight Histograms\n",
    "        2. LIME explainer\n",
    "        3. Model performance on specific features\n",
    "        4. t-SNE or PCA of Layer Activations（目前做的是2个dimensions）\n",
    "        5. Layer Weight and Activation Animations(Processing)（设想的是每一个epoch都有一个heatmap of features weights,点击begin的时候heatmap会从epoch1对应的开始变动到epoch5）\n",
    "3. Attention Head Analysis(may or may not do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0381a",
   "metadata": {},
   "source": [
    "## 1. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1ef63",
   "metadata": {},
   "source": [
    "### (i) Model accuracy & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05c39734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "\n",
    "# Create a figure object to hold the subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))  # Adjusted figsize for better visibility\n",
    "\n",
    "# First subplot for Accuracy\n",
    "ax[0].plot(history.history['accuracy'], color='green')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Second subplot for Loss\n",
    "ax[1].plot(history.history['loss'], color='green')\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Use Streamlit's function to display the figure\n",
    "st.pyplot(fig)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d484ec2",
   "metadata": {},
   "source": [
    "### (ii) ROC Curves and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d2f9a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the probabilities for the positive class\n",
    "y_pred_probs = model.predict(valid_videos)\n",
    "y_score = y_pred_probs[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, thresholds = roc_curve(valid_labels, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='seagreen', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "ax.plot([0, 1], [0, 1], color='limegreen', lw=2, linestyle='--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic')\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "# Use Streamlit's st.pyplot() to render the plot\n",
    "st.pyplot(fig)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b4518",
   "metadata": {},
   "source": [
    "## 2. Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fb5df",
   "metadata": {},
   "source": [
    "### 2.1 Confusion matrix (TF vs. TN vs. FP vs. FN) \n",
    "Result Evaluation  - Bias towards Classifying Videos to Normal Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6e2b7b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate 16 testing videos\n",
    "\n",
    "model_predict_test = model.predict(testloader)\n",
    "\n",
    "# Inspect Test Results\n",
    "\n",
    "pred = []\n",
    "normal_count = 0\n",
    "misclassified_indicies = []\n",
    "\n",
    "for i in range(16):\n",
    "    crash_conf = model_predict_test[i][0]\n",
    "    normal_conf = model_predict_test[i][1]\n",
    "    if (crash_conf > normal_conf):\n",
    "#         print(\"Prediction: Crash\", crash_conf)\n",
    "        pred.append(1)\n",
    "    else:\n",
    "#         print(\"Prediction: Normal\", normal_conf)\n",
    "        pred.append(0)\n",
    "        normal_count += 1\n",
    "#     if (i < 8):\n",
    "#         print(\"Actual:     Crash\", test_normal[i]+\".mp4\")\n",
    "#     else:\n",
    "#         print(\"Actual:     Normal\", test_crash[abs(i-8)]+\".mp4\")\n",
    "#     print()\n",
    "# print(str(abs(len(test_labels)-normal_count)) + \" videos out of 16 videos are classified as Crash\")\n",
    "# print(str(normal_count) + \" videos out of 16 videos are classified as Normal\")\n",
    "\n",
    "true = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0]\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn\n",
    "fig, ax = plt.subplots()  # Create a figure and a set of subplots\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax)\n",
    "ax.set_xlabel('Predicted Labels (0-Normal, 1-Crash)')\n",
    "ax.set_ylabel('True Labels (0-Normal, 1-Crash)')\n",
    "ax.set_title('Confusion Matrix of Predicted vs. True Labels')\n",
    "\n",
    "# If using Streamlit, use st.pyplot() to render matplotlib figures\n",
    "st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26d5d4",
   "metadata": {},
   "source": [
    "### 2.2 Feature analysis (Weight and Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights,biases = model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fee90869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_visual(display, rank_style, num_feature):\n",
    "    weights,biases = model.layers[-1].get_weights()\n",
    "    bound = math.ceil(max(abs(np.max(weights)), abs(np.min(weights)))*10)/10\n",
    "    \n",
    "    differences = abs(weights[:, 1]) - abs(weights[:, 0])\n",
    "    \n",
    "    # Rank Styles:\n",
    "    sorted_indices = [int(i) for i in range(EMBED_DIM)]\n",
    "    if (rank_style == 'Normal (Top First)'): # rank by greatest contribution to normal class\n",
    "        sorted_indices = np.argsort(weights[:, 0])[::-1]\n",
    "    elif (rank_style == 'Normal (Bottom First)'): # rank by smallest contribution to normal class\n",
    "        sorted_indices = np.argsort(weights[:, 0])\n",
    "    elif (rank_style == 'Crash (Top First)'): # rank by greatest contribution to crash class\n",
    "        sorted_indices = np.argsort(weights[:, 1])[::-1]\n",
    "    elif (rank_style == 'Crash (Bottom First)'): # rank by smallest contribution to crash class\n",
    "        sorted_indices = np.argsort(weights[:, 1])\n",
    "    \n",
    "    # Summary:\n",
    "    # (1) How many feature has tendency towards classifying model to Crash/Normal Class\n",
    "    negative_count = np.sum(differences < 0) # Normal\n",
    "    positive_count = np.sum(differences > 0) # Crash\n",
    "    \n",
    "    # (2) Greatest Crash POSITIVE Influence\n",
    "    max_crash_positive_influence = np.max(weights[:, 1][weights[:, 1] > 0])\n",
    "    \n",
    "    # (3) Smallest Crash POSITIVE Influence\n",
    "    min_crash_positive_influence = np.min(weights[:, 1][weights[:, 1] > 0])\n",
    "    \n",
    "    # (4) Greatest Normal POSITIVE Influence\n",
    "    max_normal_positive_influence = np.max(weights[:, 0][weights[:, 1] > 0])\n",
    "    \n",
    "    # (5) Smallest Normal POSITIVE Influence\n",
    "    min_normal_positive_influence = np.min(weights[:, 0][weights[:, 1] > 0])\n",
    "    \n",
    "    info = f'Total Number of Features: {EMBED_DIM}\\n{negative_count} Features Contribute Mainly to Normal Class\\n{positive_count} Features Contribute Mainly to Crash Class\\nMax Crash Positive Influence: {max_crash_positive_influence}\\nMin Crash Positive Influence: {min_crash_positive_influence}\\nMax Normal Positive Influence: {max_normal_positive_influence}\\nMin Normal Positive Influence: {min_normal_positive_influence}'\n",
    "    props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.5)\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    if (display == 'All'):\n",
    "        index = np.arange(2)\n",
    "        if (num_feature>1):\n",
    "            fig, axs = plt.subplots(num_feature, 1, figsize=(5, num_feature+0.5))\n",
    "            for i in range(num_feature):\n",
    "                axs[i].barh(index, weights[sorted_indices[i], :], 1, color=['skyblue', 'pink']) ##\n",
    "                axs[i].set_yticks(index)\n",
    "                axs[i].set_yticklabels(['Normal', 'Crash'])\n",
    "                axs[i].set_title('Feature {}'.format(sorted_indices[i]))\n",
    "                axs[i].set_xlim([-bound, bound])\n",
    "                axs[i].set_xlabel('Influence')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(5, 1.26))\n",
    "            ax.barh(index, weights[sorted_indices[0], :], 1, color=['skyblue', 'pink'])\n",
    "            ax.set_yticks(index)\n",
    "            ax.set_yticklabels(['Normal', 'Crash'])\n",
    "            ax.set_title('Feature {}'.format(sorted_indices[0]))\n",
    "            ax.set_xlim([-bound, bound])\n",
    "            ax.set_xlabel('Influence')\n",
    "    else:\n",
    "        if (num_feature>1):\n",
    "            fig, axs = plt.subplots(num_feature, 1, figsize=(5, num_feature))\n",
    "            for i in range(num_feature):\n",
    "                axs[i].barh(0, differences[sorted_indices[i]], color='pink' \\\n",
    "                            if differences[sorted_indices[i]] >= 0 else 'skyblue')\n",
    "                axs[i].set_title('Feature {}'.format(sorted_indices[i]))\n",
    "                axs[i].set_xlim([-bound, bound])\n",
    "                axs[i].set_xlabel('Influence Tendency')\n",
    "                axs[i].axes.get_yaxis().set_visible(False)\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(5, 1.1))\n",
    "            ax.barh(0, differences[sorted_indices[0]], color='pink' \\\n",
    "                    if differences[sorted_indices[0]] >= 0 else 'skyblue')\n",
    "            ax.set_title('Feature {}'.format(sorted_indices[0]))\n",
    "            ax.set_xlim([-bound, bound])\n",
    "            ax.set_xlabel('Influence Tendency')\n",
    "            ax.axes.get_yaxis().set_visible(False)\n",
    "    fig.text(1.05, 1, info, transform=fig.transFigure, fontsize=12, verticalalignment='top', bbox=props)\n",
    "#     plt.tight_layout()\n",
    "    st.pyplot(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert ipywidgets UI into Streamlit UI\n",
    "display_options = ['All', 'Tendency']\n",
    "rank_style_options = [\n",
    "    'Default',\n",
    "    'Normal (Top First)', \n",
    "    'Normal (Bottom First)', \n",
    "    'Crash (Top First)',\n",
    "    'Crash (Bottom First)'\n",
    "]\n",
    "\n",
    "# Create Streamlit UI\n",
    "display = st.selectbox('Display:', display_options)\n",
    "rank_style = st.selectbox('Rank Style:', rank_style_options)\n",
    "num_feature = st.slider('Number of Features:', 1, EMBED_DIM-1, value=3)\n",
    "\n",
    "# Call function with Streamlit widgets' values\n",
    "feature_visual(display, rank_style, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596b2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
